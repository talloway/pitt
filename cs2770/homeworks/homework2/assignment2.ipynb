{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"assignment2.ipynb","provenance":[],"collapsed_sections":["YQsvYOhY_ULp","s66Fdpuw8otK"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"753675e19ba5435d85b16757b3f0ee9c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e2e85642c0a548f9afc5ae2ae5df3511","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0cfdb7f2c6ea461c940219d9b3ffa3c9","IPY_MODEL_f778d68c9c9248d6a5178e829f242150"]}},"e2e85642c0a548f9afc5ae2ae5df3511":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0cfdb7f2c6ea461c940219d9b3ffa3c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c18345dce8ac47a1abe6fde8c9b0c382","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":553433881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":553433881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_29cc935167b341aaa35269cbe87cf9c1"}},"f778d68c9c9248d6a5178e829f242150":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5fe06fe1f45a42e4bf63a7beece76376","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 528M/528M [00:20&lt;00:00, 27.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_201b54865536455da91528f1c58c494a"}},"c18345dce8ac47a1abe6fde8c9b0c382":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"29cc935167b341aaa35269cbe87cf9c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5fe06fe1f45a42e4bf63a7beece76376":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"201b54865536455da91528f1c58c494a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"YQsvYOhY_ULp"},"source":["\n","# Part A: Loading and Using a Pretrained Network as a Feature Extractor"]},{"cell_type":"markdown","metadata":{"id":"9etLuaTNofP4"},"source":["1. Import required modules and libraries and mount Google Drive for access of data"]},{"cell_type":"code","metadata":{"id":"XHRnMXnmCdD3"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import time\n","import os\n","import copy\n","from sklearn import svm\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZM_sHc4rFHH","executionInfo":{"status":"ok","timestamp":1615748284614,"user_tz":240,"elapsed":1172,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"de3c7b02-e4cd-4d2c-9dae-3bfe268e11b6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/cs2770_hw2')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NXnXVqCGon4l"},"source":["2. Preprocess data"]},{"cell_type":"code","metadata":{"id":"61jedQzfCp3t"},"source":["# transforms.Normalize normalizes tensor values based on mean and standard deviation for RGB values\n","# [0.485, 0.456, 0.406] are the means for RGB channels\n","# [0.229, 0.224, 0.225] are the standard deviations for RGB channels\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","      transforms.Resize((224, 224)),\n","      transforms.ToTensor(),\n","      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                              \n","    ]),\n","    'val': transforms.Compose([\n","      transforms.Resize((224, 224)),\n","      transforms.ToTensor(),\n","      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","      transforms.Resize((224, 224)),\n","      transforms.ToTensor(),\n","      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LA222EA2o0FF"},"source":["3. Create data loader"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8zHroG_Hmm0","executionInfo":{"status":"ok","timestamp":1615748297252,"user_tz":240,"elapsed":13793,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"169aa089-8861-4584-ffbb-e36f2a8eee4f"},"source":["data_dir = 'hw2_data'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n","  for x in ['train', 'val', 'test']}\n","\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8, shuffle=True, num_workers=4)\n","  for x in ['train', 'val', 'test']}\n","  \n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n","class_names = image_datasets['train'].classes"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"20T8ZIgSo4d6"},"source":["4. Load pretrained CNN model and use `VGG16_Feature_Extraction` model to extract features of images"]},{"cell_type":"code","metadata":{"id":"U5B0uwgOhOD0"},"source":["class VGG16_Feature_Extraction(torch.nn.Module):\n","  def __init__(self):\n","    super(VGG16_Feature_Extraction, self).__init__()\n","    VGG16_Pretrained = models.vgg16(pretrained=True)\n","    self.features = VGG16_Pretrained.features\n","    self.avgpool = VGG16_Pretrained.avgpool\n","    self.feature_extractor = nn.Sequential(*[VGG16_Pretrained.classifier[i] for i in range(6)])\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = self.avgpool(x)\n","    x = torch.flatten(x, 1)\n","    x = self.feature_extractor(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SG7NX3pIiOGz","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["753675e19ba5435d85b16757b3f0ee9c","e2e85642c0a548f9afc5ae2ae5df3511","0cfdb7f2c6ea461c940219d9b3ffa3c9","f778d68c9c9248d6a5178e829f242150","c18345dce8ac47a1abe6fde8c9b0c382","29cc935167b341aaa35269cbe87cf9c1","5fe06fe1f45a42e4bf63a7beece76376","201b54865536455da91528f1c58c494a"]},"executionInfo":{"status":"ok","timestamp":1615748319125,"user_tz":240,"elapsed":35651,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"8f7cc8ad-a24d-483d-8521-3f36902dd3b8"},"source":["model = VGG16_Feature_Extraction()\n","device = 'cuda:0'\n","model = model.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"753675e19ba5435d85b16757b3f0ee9c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0kvzgLwypLHi"},"source":["5. Use model to extract features of images"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4tFuWY0ZiPcX","executionInfo":{"status":"ok","timestamp":1615748809467,"user_tz":240,"elapsed":525987,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"c64cb3ff-a8d3-4480-e892-80f94d823bef"},"source":["image_features = {}\n","image_labels = {}\n","\n","for phase in ['train', 'test']:\n","  for inputs, labels in dataloaders[phase]:\n","    inputs = inputs.to(device)\n","    model_prediction = model(inputs)\n","    model_prediction_numpy = model_prediction.cpu().detach().numpy()\n","    if (phase not in image_features):\n","      image_features[phase] = model_prediction_numpy\n","      image_labels[phase] = labels.numpy()\n","    else:\n","      image_features[phase] = np.concatenate((image_features[phase], model_prediction_numpy), axis=0)\n","      image_labels[phase] = np.concatenate((image_labels[phase], labels.numpy()), axis=0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"wH-Bvg96jW0a"},"source":["6. Train the network on the training data after scaling it"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9Bx8jETGsMj","executionInfo":{"status":"ok","timestamp":1615748933697,"user_tz":240,"elapsed":650211,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"13675892-dee9-4cbc-e1e4-0c3af8e12210"},"source":["clf = make_pipeline(StandardScaler(), svm.LinearSVC(random_state=0, tol=1e-5))\n","clf.fit(image_features['train'], image_labels['train'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('standardscaler',\n","                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n","                ('linearsvc',\n","                 LinearSVC(C=1.0, class_weight=None, dual=True,\n","                           fit_intercept=True, intercept_scaling=1,\n","                           loss='squared_hinge', max_iter=1000,\n","                           multi_class='ovr', penalty='l2', random_state=0,\n","                           tol=1e-05, verbose=0))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"EnJ9tgC1pUPN"},"source":["7. Test network on test set and report accuracy. Display confusion matrix to see which classes are being incorrectly predicted the most. \n","\n","  Some of the most often mispredicted labels are:\n","*   Cars are being labeled as horses.\n","*   Tables are being labeled as TV monitors, sofas, or potted plants.\n","*   People are being labeled as buses and sofas.\n","*   Sofas are being labeled as people.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bua2N6uplVWM","executionInfo":{"status":"ok","timestamp":1615748933699,"user_tz":240,"elapsed":650206,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"8a4cb267-461a-4197-b36b-e997e023a45d"},"source":["correct = 0\n","\n","for p, t in zip(clf.predict(image_features['test']), image_labels['test']):\n","  if p == t:\n","    correct += 1\n","\n","print('Accuracy of SVM: ' + str(round((100 * correct / image_labels['test'].size), 2)) + '%.')\n","print(confusion_matrix(clf.predict(image_features['test']), image_labels['test']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of SVM: 53.57%.\n","[[22  0  0  1  0  2  1  1  0  0  0  0  0  0  0  0  0  0  1  1]\n"," [ 0 13  0  0  0  0  0  0  2  0  1  2  0  1  0  2  1  0  2  0]\n"," [ 0  0 20  0  0  0  0  1  0  1  0  0  0  0  0  0  2  0  0  0]\n"," [ 1  1  0 21  0  1  0  0  0  1  0  2  2  0  1  0  1  0  1  0]\n"," [ 0  0  0  1  5  0  0  1  1  0  3  1  0  0  3  2  0  0  1  1]\n"," [ 0  0  1  0  0 14  0  0  0  0  0  0  0  0  1  0  0  0  0  0]\n"," [ 0  1  0  0  0  3 14  0  1  1  1  0  0  2  3  0  1  0  1  0]\n"," [ 0  0  2  0  0  0  0 18  0  0  0  1  0  0  0  1  0  1  0  1]\n"," [ 1  0  0  0  2  0  1  0  4  2  3  1  0  0  2  1  0  8  0  3]\n"," [ 0  0  0  0  0  0  0  0  0  5  0  1  2  0  0  0  1  0  0  0]\n"," [ 1  1  0  0  5  0  0  0  5  0 11  1  0  0  1  4  0  2  0  2]\n"," [ 0  0  0  0  1  0  0  1  0  1  0 11  0  0  1  2  0  1  0  0]\n"," [ 0  0  0  0  1  0  0  0  0  4  0  0 15  0  1  1  0  0  0  0]\n"," [ 0  1  1  0  0  0  1  0  0  0  1  0  1 21  2  0  0  0  0  0]\n"," [ 0  3  1  0  6  0  5  0  2  0  2  3  2  1  8  1  0  2  1  1]\n"," [ 0  2  0  1  1  0  0  1  0  0  2  1  0  0  1  4  0  1  0  2]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0 11  1  0  0]\n"," [ 0  1  0  0  2  0  2  1  4  0  1  1  0  0  1  3  0  9  1  2]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0]\n"," [ 0  2  0  1  2  1  1  1  6  0  0  0  0  0  0  4  0  0  0 12]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s66Fdpuw8otK"},"source":["# Part B: Training and Testing the CNN on our Dataset"]},{"cell_type":"markdown","metadata":{"id":"uYsaCVB-P6TQ"},"source":["**Preparing the network**\n","\n","1. Load VGG16 with pretrained weights from ImageNet.\n","2. Extract the number of input features for the last fully connected layer of the model.\n","3. Replace the last fully connected layer with a new layer."]},{"cell_type":"code","metadata":{"id":"mXZ0D785P28g"},"source":["model = models.vgg16(pretrained=True)\n","num_ftrs = model.classifier[6].in_features\n","model.classifier[6] = nn.Linear(num_ftrs, len(class_names))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pGZgGBirQydC"},"source":["**Steps before starting training**\n","\n","4. Set number of epochs to 25.\n","5. Send the model to the CUDA device.\n","6. Specify the criterion for evaluating the trained model.\n","7. Set the optimizer, learning rate, and momentum.\n","8. Create a scheduler to control the way that the learning rate changes during the training process."]},{"cell_type":"markdown","metadata":{"id":"qlEscWBYSXtv"},"source":["**Training**\n","\n","9. Save the initial model weight as the best model weight and set the best accuracy as zero.\n","10. Iterate over the epochs.\n","11. Iterate over the train and validation set.\n","12. Use the dataloader from previous steps to get a minibatch of images and their corresponding labels.\n","13. Initialize the gradient vector to all zeroes.\n","14. Use the current model weight for prediction and backpropagating the prediction loss. \n","15. Update the scheduler status.\n","16. Compute loss and accuracy of epoch.\n","17. Check whether the accuracy of classification is better than the best accuracy so far to save the best model parameters."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yb_a0cFXb2G8","executionInfo":{"status":"ok","timestamp":1615749234923,"user_tz":240,"elapsed":301196,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"db2a39e9-9652-4246-8797-b8b8ffbe6112"},"source":["model = model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","\n","epochs = 25\n","learning_rate = 0.001\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)      \n","\n","best_model_wts = copy.deepcopy(model.state_dict())\n","best_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","  all_batchs_loss = 0.0\n","  all_batchs_corrects = 0.0\n","\n","  for phase in ['train', 'val']:\n","    if phase == 'train':\n","      model.train()\n","    else:\n","      model.eval()\n","\n","    for inputs, labels in dataloaders[phase]: # iterating over batches\n","      inputs = inputs.to(device)\n","      labels = labels.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      with torch.set_grad_enabled(phase == 'train'):\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        loss = criterion(outputs, labels)\n","        # print(loss)\n","        if phase == 'train':\n","          loss.backward()\n","          optimizer.step()\n","      \n","        all_batchs_loss += loss.item() * inputs.size(0)\n","        all_batchs_corrects += torch.sum(preds == labels.data)\n","\n","    if phase == 'train':\n","      scheduler.step()\n","    \n","    epoch_loss = all_batchs_loss / dataset_sizes[phase]\n","    epoch_acc = all_batchs_corrects.double() / dataset_sizes[phase]\n","\n","    if phase == 'val' and epoch_acc > best_acc:\n","      best_acc = epoch_acc\n","      best_model_wts = copy.deepcopy(model.state_dict())\n","      torch.save(best_model_wts, 'best_model_weight.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"6XIOYQPvmEtY"},"source":["**Testing**\n","\n","18.   Prepare the model in the same way it was prepared for training and load the best model weight saved in training.\n","19.   Set model to `eval` and phase to `'test'`. \n","20. Go through the test set, predict the category of images, and compute the number of correctly classified images.\n","21. Compute accuracy over all data.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVc-qVwnl7zL","executionInfo":{"status":"ok","timestamp":1615749288007,"user_tz":240,"elapsed":6893,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"a5afcfb0-95ab-41bf-95b3-9e9a8165ac73"},"source":["model = models.vgg16()\n","num_ftrs = model.classifier[6].in_features\n","model.classifier[6] = nn.Linear(num_ftrs, 20)\n","model = model.to(device)\n","model.load_state_dict(torch.load('best_model_weight.pth'))\n","\n","model.eval()\n","phase = 'test'\n","\n","all_labels = []\n","all_preds = []\n","for inputs, labels in dataloaders[phase]:\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","  outputs = model(inputs)\n","  _, preds = torch.max(outputs, 1)\n","  all_batchs_corrects += torch.sum(preds == labels.data)\n","  all_preds.append(preds.cpu().data.numpy())\n","  all_labels.append(labels.cpu().data.numpy())\n","\n","all_preds = np.concatenate(all_preds)\n","all_labels = np.concatenate(all_labels)\n","\n","test_acc = all_batchs_corrects.double() / dataset_sizes[phase]\n","\n","conf = confusion_matrix(all_labels, all_preds)\n","\n","print('Confusion matrix: ')\n","print(conf)\n","print(f\"Testing accuracy from given formula: {test_acc}\")\n","print(f\"Testing accuracy: {np.trace(conf)/np.sum(conf)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["Confusion matrix: \n","[[25  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0 12  0  0  0  0  4  0  2  0  0  0  0  5  1  0  0  0  0  1]\n"," [ 1  1 17  1  1  0  0  0  0  1  0  1  0  0  0  1  0  0  1  0]\n"," [ 1  0  0 17  1  0  2  0  0  0  0  0  0  1  1  1  0  0  1  0]\n"," [ 0  0  0  0 13  0  0  0  2  0  4  1  0  0  3  0  0  1  0  1]\n"," [ 1  0  0  0  0 17  2  0  0  0  0  0  0  0  0  0  0  0  1  0]\n"," [ 1  0  0  0  0  0 19  0  1  0  0  0  0  2  1  0  0  0  1  0]\n"," [ 0  0  0  0  1  0  0 22  0  0  0  1  0  0  0  1  0  0  0  0]\n"," [ 0  0  0  0  3  0  1  1 10  0  5  1  0  1  1  0  0  1  0  1]\n"," [ 0  0  0  0  0  0  0  0  1 11  0  0  1  1  1  0  0  0  0  0]\n"," [ 0  0  0  0 11  0  0  0  2  0 10  0  0  0  1  1  0  0  0  0]\n"," [ 0  0  1  0  1  0  1  0  1  0  0 18  0  1  1  0  1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  2  0  0 20  1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  2  0  0  0  0  0  0 22  0  0  0  0  1  0]\n"," [ 2  0  0  0  6  0  2  0  1  0  0  1  1  0 11  0  0  1  0  0]\n"," [ 0  1  0  0  1  0  1  0  4  0  3  1  0  2  2  9  0  1  0  0]\n"," [ 0  0  1  1  0  0  1  0  0  3  0  0  0  0  0  0 10  0  1  0]\n"," [ 0  0  0  0  4  0  0  1  4  0  1  2  0  0  3  2  0  8  0  0]\n"," [ 0  0  0  0  0  0  2  0  0  0  0  0  0  0  1  0  0  0 22  0]\n"," [ 0  0  0  0  3  0  0  0  4  0  0  0  0  0  1  1  0  1  0 15]]\n","Testing accuracy from given formula: 7.962184873949579\n","Testing accuracy: 0.6470588235294118\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zim56s60b-6I"},"source":["22. Repeating with Different Hyperparameters\n","\n","Accuracy with following variations of hyperparameters **(Base case is 25 epochs, learning rate of 0.001, and SGD optimizer)**:\n","*   Epochs = 20: 0.6659\n","*   Epochs = 25: 0.6197\n","*   Epochs = 30: 0.6554\n","\n","\n","*   Learning Rate = 0.001: 0.6197 \n","*   Learning Rate = 0.0005: 0.6870\n","\n","\n","*   Optimizer = SGD: 0.6197\n","*   Optimizer = Adam: 0.0525"]},{"cell_type":"markdown","metadata":{"id":"a27OOEBo-d6z"},"source":["# Part C: Object Detection Training and Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKSiOx3lzPCa","executionInfo":{"status":"ok","timestamp":1616516577954,"user_tz":240,"elapsed":32765,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"a436094e-eb34-4027-bb79-d4439f7ac349"},"source":["import os\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/cs2770_hw2')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nhiWgMhduorZ","executionInfo":{"status":"ok","timestamp":1616516590773,"user_tz":240,"elapsed":4274,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}}},"source":["import torch\n","import torchvision\n","import Required_Files.utils as utils\n","\n","from Required_Files.coco_utils import get_coco_api_from_dataset\n","from Required_Files.coco_eval import CocoEvaluator\n","import copy\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from Required_Files.PennFudanDataset import PennFudanDataset\n","from Required_Files.pascal_dataset import PASCALDataset\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","import json"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkhrYS6Xr-CS","executionInfo":{"status":"ok","timestamp":1616519572377,"user_tz":240,"elapsed":3628,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"338088bc-85c4-4480-902d-043c94df6700"},"source":["# train = PASCALDataset('PASCAL/train')\n","# test = PASCALDataset('PASCAL/test')\n","# val = PASCALDataset('PASCAL/val')\n","\n","train = PennFudanDataset('PennFudanPed_hw3/train')\n","test = PennFudanDataset('PennFudanPed_hw3/test')\n","val = PennFudanDataset('PennFudanPed_hw3/val')\n","\n","data_loader_train = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\n","data_loader_test = torch.utils.data.DataLoader(test, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\n","data_loader_val = torch.utils.data.DataLoader(val, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oT6xxSfvuYeP","executionInfo":{"status":"ok","timestamp":1616519574969,"user_tz":240,"elapsed":1135,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}}},"source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","device = 'cuda:0'\n","model = model.to(device) "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pGwAbbqN17s","executionInfo":{"status":"ok","timestamp":1616519702948,"user_tz":240,"elapsed":125221,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"12da8674-43a3-4cb4-e461-69f25de6f5f0"},"source":["num_epochs = 5\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","best_mAP = 0.0\n","torch.set_grad_enabled(True) # might be able to delete this and just make sure that training code executes before testing code\n","\n","for epoch in range(num_epochs):\n","  for phase in ['train', 'val']:\n","    if phase == 'train':\n","      model.train() # just putting it in training mode and eval mode, not actually doing anything\n","    if phase == 'val':\n","      model.eval()\n","\n","    if phase == 'train':\n","      for images, targets in data_loader_train: # batch - small number of images which are run in parallel on GPU\n","\n","        optimizer.zero_grad() \n","\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        loss_dict = model(images, targets)\n","\n","        with torch.set_grad_enabled(phase == 'train'):\n","            loss = sum(loss_dict.values())\n","            loss.backward()\n","            optimizer.step()\n","    \n","      scheduler.step()\n","    \n","    if phase == 'val':\n","      coco = get_coco_api_from_dataset(data_loader_val.dataset)\n","      iou_types = [\"bbox\"]\n","      coco_evaluator = CocoEvaluator(coco, iou_types)\n","    \n","      for images, targets in data_loader_val:\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        outputs = model(images) # send in images and get labels\n","        # outputs = list(output.cpu() for output in outputs) # how to get this to the CPU?\n","        outputs = [{k: v.cpu() for k, v in o.items()} for o in outputs]\n","\n","        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n","        coco_evaluator.update(res) # basically adding to the running total in the same way as was done in part b\n","\n","      coco_evaluator.synchronize_between_processes()\n","      coco_evaluator.accumulate()\n","      coco_evaluator.summarize()\n","      mAP = coco_evaluator.coco_eval['bbox'].stats[0] # accuracy over epoch \n","\n","      if mAP > best_mAP:\n","        best_mAP = mAP\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        torch.save(best_model_wts, 'best_model_weight.pth')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["creating index...\n","index created!\n","Accumulating evaluation results...\n","DONE (t=0.01s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.859\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.965\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.606\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.884\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.535\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.908\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.908\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.783\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.924\n","creating index...\n","index created!\n","Accumulating evaluation results...\n","DONE (t=0.01s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.971\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.971\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.572\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.524\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.896\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.896\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.817\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.907\n","creating index...\n","index created!\n","Accumulating evaluation results...\n","DONE (t=0.01s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.849\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.971\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.971\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.568\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.872\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.524\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.894\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.894\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.800\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.907\n","creating index...\n","index created!\n","Accumulating evaluation results...\n","DONE (t=0.01s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.853\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.973\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.973\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.569\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.878\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.525\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.898\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.898\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.783\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.913\n","creating index...\n","index created!\n","Accumulating evaluation results...\n","DONE (t=0.01s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.862\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.973\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.973\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.635\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.883\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.531\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.908\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.908\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.833\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.918\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I6h2pVAp6XIl","executionInfo":{"status":"ok","timestamp":1616519703370,"user_tz":240,"elapsed":412,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}}},"source":["# function for calculating mAP scores\n","def calculate_mAP(target, output):\n","  distinct_classes = []\n","  threshold = 0.5\n","  sum = 0\n","\n","  # find distinct classes in target\n","  for label in target[\"labels\"]:\n","    if label not in distinct_classes:\n","      distinct_classes.append(label)\n","\n","  for cls in distinct_classes:\n","    tp = 0\n","    fp = 0\n","\n","    for label, score in zip(output[\"labels\"], output[\"scores\"]):\n","      if label == cls and score > threshold:\n","        tp += 1\n","      elif label == cls:\n","        fp += 1\n","\n","    if tp == 0 and fp == 0:\n","      sum += 0\n","    else:\n","      sum += (tp / (tp + fp))\n","\n","  return sum / len(distinct_classes)\n","\n","\n","def calculate_IOU(pred, gt):\n","  if pred[0] < gt[0]:\n","    box_a = pred\n","    box_b = gt    \n","  else:\n","    box_a = gt\n","    box_b = pred\n","\n","  x0_a, y0_a, x1_a, y1_a = box_a[0], box_a[1], box_a[2], box_a[3]\n","  x0_b, y0_b, x1_b, y1_b = box_b[0], box_b[1], box_b[2], box_b[3]\n","\n","  if (x0_b > x0_a and x0_b < x1_a) or (y0_b > y0_a and y0_b < y1_a):\n","    i_area = (x1_a - x0_b) * (y1_a - y0_b)\n","    u_area = (x1_a - x0_a) * (y1_a - y0_a) + (x1_b - x0_b) * (y1_b - y0_b) - i_area\n","    \n","    return (i_area / u_area)\n","\n","  return 0\n","  "],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UyDHR_jVIeQJ"},"source":["I think there's something wrong with calculating the scores for some of the outputs, but I didn't have time to go super in-depth to figure it out. Other than that, it seems to draw the bounding boxes correctly."]},{"cell_type":"code","metadata":{"id":"W2x0oPZNKind","executionInfo":{"status":"ok","timestamp":1616519703373,"user_tz":240,"elapsed":409,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}}},"source":["# function to draw bounding boxes on top n images sorted by mAP score (n=20)\n","from PIL import Image, ImageDraw, ImageFont\n","from torchvision.transforms import ToPILImage\n","\n","def draw_bbs(mAP_scores, scores_list):\n","  n = 20\n","  scores_list.sort(reverse=True)\n","  min_value = scores_list[n-1]\n","\n","  count = 0\n","\n","  for image in mAP_scores:\n","    if count == n:\n","      return\n","      \n","    if mAP_scores[image]['score'] < min_value:\n","      continue\n","\n","    # get the image and draw its bounding box(es)\n","    im = ToPILImage()(image)\n","    draw = ImageDraw.Draw(im)\n","\n","    for box, label, gt in zip(mAP_scores[image]['boxes'], mAP_scores[image]['label'], mAP_scores[image]['targets']):\n","      arr = box.cpu().numpy()\n","      draw.rectangle(arr, outline='blue', width=2)\n","      draw.text((10, 10), 'class label: ' + str(label.cpu().numpy()))\n","      draw.text((10, 30), 'score: ' + str(calculate_IOU(arr, gt.cpu().numpy())))\n","    \n","    #im.save('submission_files/part_c/pascal_img-' + str(count+1) + '.jpg')\n","    im.save('submission_files/part_c/pennfudanped_img-' + str(count+1) + '.jpg')\n","    count += 1"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EenBqnQeu6FD","executionInfo":{"status":"ok","timestamp":1616519722500,"user_tz":240,"elapsed":19527,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}},"outputId":"8986c71e-3d66-41d5-c8dc-20beb8c48f9f"},"source":["# testing\n","model.eval()\n","torch.set_grad_enabled(False)\n","mAP_scores = {}\n","scores_list = []\n","\n","# load the model with the best weights\n","model = model.to(device) \n","model.load_state_dict(torch.load('best_model_weight.pth')) \n","\n","coco = get_coco_api_from_dataset(data_loader_test.dataset)\n","iou_types = [\"bbox\"]\n","coco_evaluator = CocoEvaluator(coco, iou_types)\n","\n","for images, targets in data_loader_test:\n","  images = list(image.to(device) for image in images)\n","  targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","  outputs = model(images)  \n","\n","  res = {}                                       \n","\n","  for image, target, output in zip(images, targets, outputs):\n","    score = calculate_mAP(target, output)\n","    scores_list.append(score)\n","    mAP_scores[image] = {'label': target[\"labels\"], 'score': score, 'boxes': output[\"boxes\"], 'targets': target[\"boxes\"]}\n","\n","    res[target[\"image_id\"].item()] = output \n","\n","  coco_evaluator.update(res)\n","\n","coco_evaluator.synchronize_between_processes()\n","coco_evaluator.accumulate()\n","coco_evaluator.summarize()\n","mAP = coco_evaluator.coco_eval['bbox'].stats[0]\n","print('Overall mAP: ' + str(mAP))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["creating index...\n","index created!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["Accumulating evaluation results...\n","DONE (t=0.01s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.825\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.968\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.924\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.625\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.459\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.858\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.371\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.871\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.871\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.892\n","Overall mAP: 0.8250616917682003\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cmipQaXlQmQV","executionInfo":{"status":"ok","timestamp":1616519723127,"user_tz":240,"elapsed":20147,"user":{"displayName":"Avery Peiffer","photoUrl":"","userId":"10747913474217927702"}}},"source":["draw_bbs(mAP_scores, scores_list)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5klOsPSAUT-v"},"source":["The model's accuracy seems to be around 60% for the PASCAL dataset and 75% for the Penn Fudan dataset. I experimented with different learning rates and epochs for the models."]}]}