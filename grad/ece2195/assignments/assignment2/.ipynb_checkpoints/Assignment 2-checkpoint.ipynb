{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h2>2221 ECE 2195 SEC1305 SPECIAL TOPICS: COMPUTERS</h2>\n",
    "<h1 style=\"font-size: 250%;\">Assignment #2</h1>\n",
    "<h3>Total points: 120 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <h3>  Problem #1. Classification </h3> \n",
    " ### [20 points]\n",
    " \n",
    "**Do not write a code for this part**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-1</h4>  <br>\n",
    "Assume we have K classes to be classified with one feature $(x)$. The prior probability of\n",
    "class $k$ is $ùúã_{k} = ùëÉ(ùëå = ùëò)$. Assume that the feature in class k has Gaussian distribution of\n",
    "mean $Œº_{k}$ and variance $œÉ^2 (ùí©(Œº,ùúé^{2}))$.The variance is the same for all classes. \n",
    "\n",
    "<br>1. Prove that the Bayes‚Äô classifier (that chooses class k with largest $ùëÉ(ùëå = ùëò|ùë•))$ is equivalent to assigning an observation to the class for which the discriminant function $ùõø_{k}(x)$ is\n",
    "maximized, where \n",
    "\\begin{array} \\\\\n",
    "ùõø_{k}(x) = x\\frac{\\mu _{k}}{\\sigma ^{2}}- \\frac{\\mu_{2}^{k}}{2\\sigma ^{2}}+ log(\\pi _{k})\n",
    "\\end{array}\n",
    "<br>2. What is the name of this classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $p_x(k) = p(x|y = k)*p(y=k) = \\pi_k * \\frac{1}{\\sqrt{2\\pi}\\sigma_k} * e^{-\\frac{1}{2}\\frac{(x-\\mu_k)}{\\sigma_k}^2}$\n",
    "\n",
    "   $log(p_x(k)) = log \\pi_k - log{\\sqrt{2\\pi}\\sigma_k} - \\frac{1}{2}\\frac{(x-\\mu_k)}{\\sigma_k}^2$\n",
    "   \n",
    "   Since the $log{\\sqrt{2\\pi}\\sigma_k}$ term does not depend on k, it isn't really necessary for the overall calculation and can be removed.\n",
    "   \n",
    "   $log(p_x(k)) = log \\pi_k - \\frac{1}{2\\sigma^2} [x^2 - 2x\\mu_k + u_k^2]$\n",
    "   \n",
    "   $log(p_x(k)) = log \\pi_k - \\frac{x^2}{2\\sigma^2} + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}$\n",
    "   \n",
    "   Since the $\\frac{x^2}{2\\sigma^2}$ term does not depend on k, it can also be removed. Also, the $log(p_x(k))$ can be reduced to $p_x(k)$ since it is a probability measure. \n",
    "   \n",
    "   $p_x(k) = \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log \\pi_k $\n",
    "   \n",
    "   \n",
    "\n",
    "2. This is the LDA classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-2 </h4>\n",
    "\n",
    "Extend **Problem #1-1** to include **p** features. With features from each class drawn from a\n",
    "Gaussian distribution with mean vector $Œº_{k}$ and covariance matrix $Œ£_{k}$ (which is now\n",
    "different for each class), please answer the following questions.\n",
    "1. What is the discriminant function that maximizes **ùëÉ(ùëå = ùëò|ùë•)**. \n",
    "2. Is the relationship with the feature vector **x** linear?\n",
    "3. What is this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $ùõø_{k}(x) = -\\frac{1}{2}x^{T}Œ£_{k}^{-1}x + x^{T}Œ£_{k}^{-1}Œº_{k} -\\frac{1}{2}Œº_{k}^{T}Œ£_{k}^{-1}Œº_{k} -\\frac{1}{2}log|Œ£_{k}| + log \\pi_k$\n",
    "2. No, the relationship with the feature vector is quadratic.\n",
    "3. QDA classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-3</h4>\n",
    "\n",
    "Explain the Bias-variance trade-off in choosing between LDA and QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a simpler estimator, which means that it has lower variance but higher bias (more likely to underfit). QDA is more complex, meaning it is more susceptible to high variance and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <h3>  Problem #2. Regularization    </h3>\n",
    "### [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-1.Logistic Regression with Ridge Regularization</h4>\n",
    "\n",
    "In this part, you should download and analyze the Wisconson **\"breast_cancer\"** dataset. <br>\n",
    "\n",
    "1. Fit logistic regression model using ridge regularization with different values of  C = 0.1, 1, 5, 10, 50, 100, and 1000 (Note that C is the LogisticRegression function argument). For each value, report the estimated coefficients for the fitted model (do not just print summary, make a table with feature names and estimated coefficients)\n",
    "\n",
    "2. What happens to the coefficients as you increase C?\n",
    "\n",
    "3. What happens to the flexibility of the model as you increase C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0.1       -0.351363     -0.399586       -0.344433  -0.334798        -0.182925   \n",
      "1.0       -0.297929     -0.580564       -0.310941  -0.377129        -0.119842   \n",
      "5.0        0.070577     -0.732883       -0.082704  -0.139309         0.108164   \n",
      "10.0       0.359470     -0.802108        0.045682   0.105858         0.200426   \n",
      "50.0       1.585355     -0.851347        0.353257   0.965456         0.384810   \n",
      "100.0      2.521606     -0.869261        0.545690   1.488781         0.502454   \n",
      "1000.0     7.360870     -1.142645        1.595153   4.004984         1.799400   \n",
      "\n",
      "        mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0.1            -0.036636       -0.312717            -0.417250      -0.204699   \n",
      "1.0             0.428555       -0.711311            -0.853712      -0.466882   \n",
      "5.0             1.270624       -1.443909            -1.412200      -0.739543   \n",
      "10.0            2.021216       -2.043171            -1.752456      -0.886876   \n",
      "50.0            6.137594       -4.547543            -3.412751      -1.466294   \n",
      "100.0           9.425053       -6.307050            -5.054598      -1.921750   \n",
      "1000.0         26.025405      -14.780138           -16.460757      -4.621831   \n",
      "\n",
      "        mean fractal dimension  ...  worst radius  worst texture  \\\n",
      "0.1                   0.193763  ...     -0.480697      -0.493624   \n",
      "1.0                   0.117625  ...     -0.916264      -0.917267   \n",
      "5.0                  -0.286970  ...     -1.547688      -1.355985   \n",
      "10.0                 -0.535354  ...     -2.022954      -1.688031   \n",
      "50.0                 -1.723314  ...     -3.793722      -3.150481   \n",
      "100.0                -2.718891  ...     -5.142930      -4.173200   \n",
      "1000.0               -7.751378  ...    -12.741087      -9.573320   \n",
      "\n",
      "        worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
      "0.1           -0.453026   -0.422424         -0.326463          -0.165815   \n",
      "1.0           -0.815983   -0.865392         -0.455392           0.103474   \n",
      "5.0           -1.361910   -1.498423         -0.418987           0.566285   \n",
      "10.0          -1.834424   -1.949891         -0.443851           0.826138   \n",
      "50.0          -3.949646   -3.838268         -0.737713           1.381803   \n",
      "100.0         -5.440092   -5.295179         -0.907154           1.570331   \n",
      "1000.0       -11.827072  -12.788752         -1.830622           2.099928   \n",
      "\n",
      "        worst concavity  worst concave points  worst symmetry  \\\n",
      "0.1           -0.379817             -0.497140       -0.372748   \n",
      "1.0           -0.830093             -0.984452       -0.592004   \n",
      "5.0           -1.511077             -1.237271       -0.771360   \n",
      "10.0          -2.032735             -1.211574       -0.852158   \n",
      "50.0          -4.156143             -0.443978       -0.794565   \n",
      "100.0         -5.527787              0.327671       -0.634123   \n",
      "1000.0       -12.456114              4.645238        0.323974   \n",
      "\n",
      "        worst fractal dimension  \n",
      "0.1                   -0.174145  \n",
      "1.0                   -0.610870  \n",
      "5.0                   -1.586988  \n",
      "10.0                  -2.304269  \n",
      "50.0                  -5.053565  \n",
      "100.0                 -6.944077  \n",
      "1000.0               -16.541509  \n",
      "\n",
      "[7 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "DataCancer=load_breast_cancer()\n",
    "\n",
    "X = DataCancer.data\n",
    "Y = DataCancer.target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_t = scaler.transform(X_train)\n",
    "X_test_t = scaler.transform(X_test)\n",
    "\n",
    "# Your code here\n",
    "C = [0.1, 1, 5, 10, 50, 100, 1000]\n",
    "coefs = []\n",
    "fnames = DataCancer.feature_names\n",
    "\n",
    "for c in C:\n",
    "    LogRegModel = LogisticRegression(penalty='l2', C=c, solver='lbfgs', max_iter=1000)\n",
    "    LogRegModel.fit(X_train_t, Y_train)\n",
    "    \n",
    "    coefs.append(LogRegModel.coef_[0])\n",
    "\n",
    "df = pd.DataFrame(coefs, index=C, columns=fnames)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The coefficients increase in magnitude as C is increased.\n",
    "3. It increases the flexibility of the model because there is less regularization. As the coefficients go to 0 (C goes to 0), the model becomes more simple and less flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3>  Problem #3. Logistic Regression and Unbalanced Datasets  </h3> \n",
    "### [20 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We fit a logistic regression model to predict the probability that an individual will default on his/her credit card balance. We used the total balance (single feature) to fit the model and got the results shown in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-1. </h4> Prediciton with Logistic regression <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Table|Coefficient|Std.error|Z-statistic|P-Value|\n",
    "|:--:|:--:|:--:|:--:|:--:|\n",
    "|Intercept|-10.6513|0.3612|-29.5|<0.0001|\n",
    "|balance|0.0055|0.002|24.9|<0.0001|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the parametric model used in logistic regression? Please write down the formula.\n",
    "- What is the class label of an individual with a balance equals to 15,000 dollar? What is the class label of an individual with balance equals to 800 dollar? (Write a python function which takes two inputs (feature, model_parameters), and returns the class labels for the data). Default is class 1 and non-default is class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $f(X) = \\frac{e^{-10.6513 + 0.0055 * balance}}{1 + e^{-10.6513 + 0.0055 * balance}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For balance = 15000, individual is predicted to default\n",
      "For balance = 800, individual is predicted to not default\n"
     ]
    }
   ],
   "source": [
    "def predict_log_reg(feature, model_parameters):\n",
    "    b0 = model_parameters[0]\n",
    "    b1 = model_parameters[1]\n",
    "    \n",
    "    term = np.e ** (b0 + b1*feature)\n",
    "    res = term / (1 + term)\n",
    "    \n",
    "    if res >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "params = [-10.6513, 0.0055]\n",
    "features = [15000, 800]\n",
    "\n",
    "for feature in features:\n",
    "    label = predict_log_reg(feature, params)\n",
    "    \n",
    "    if label == 1:\n",
    "        print(f'For balance = {feature}, individual is predicted to default')\n",
    "    else:\n",
    "        print(f'For balance = {feature}, individual is predicted to not default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-2. </h4>\n",
    "\n",
    "a) The coefficients of logistic regression are obtained by maximizing the likelihood function\n",
    "\n",
    "\\begin{array} \\\\\n",
    "l(\\beta) = \\prod_{i:y_{i}=1} P(y_{i} = 1|x)\\prod_{i{}':y_{{i}'}=0} (1-P(y_{{i}'} = 1|x))\n",
    "\\end{array}\n",
    "Please show that maximizing the\n",
    "likelihood function is equivalent to minimizing the cross entropy\n",
    "\n",
    "\n",
    "Let $n$ be the number training examples. \n",
    "\n",
    "b) Is there a closed-form solution for coefficients?\n",
    "\n",
    "c) Assume we use gradient method (gradient ascent or descent) to find the coefficients. Derive the gradient, and describe how the optimal coefficients can be obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) $log(l(\\beta)) = Œ£_{i:y:=1}log(P(y_{i} = 1|x)) + Œ£_{i' : y_{i'} = 0}log(1-P(y_{i} = 1|x))$\n",
    "\n",
    "   $log(l(\\beta)) = Œ£_{i}[y_{i} log(P(y_{i} = 1|x)) +(1 - y_{i}) log (1 - P(y_{i} = 1 | x))]$\n",
    "   \n",
    "   Maximizing $log(l(\\beta))$ is the same as minimizing $-log(l(\\beta))$, which is the above result multiplied by -1. The above result is also the cross-entropy function.\n",
    "\n",
    "b) There is no closed-form solution for the coefficients because the sigmoid function is nonlinear.\n",
    "\n",
    "c) Gradient ascent: $\\beta_{j} := \\beta_{j} + \\lambda \\frac{dJ(\\beta)}{d(\\beta_{j})}$\n",
    "\n",
    "   $ J(\\beta) = Œ£[y_{i} log(f(x_{i})) + (1-y_{i})log(1-f(x_{i}))]$\n",
    "   \n",
    "   $ \\frac{dJ(\\beta)}{d(\\beta_{j})} = Œ£[\\frac{y_{i}}{f(x_{i})} \\frac{d}{d\\beta_{j}} f(x_{i}) - \\frac{1-y_{i}}{1-f(x_{i})} \\frac{d}{d\\beta_{j}} (1-f(x_{i}))]$\n",
    "   \n",
    "   $ = \\frac{y_{i}(1 - f(x_{i})) - (1 - y_{i})(f(x_{i}))}{f(x_{i})(1-f(x_{i}))} * f(x_{i})(1-f(x_{i}))x_{ij}$\n",
    "   \n",
    "   $ = (y_{i} - f(x_{i}))x_{ij}$\n",
    "   \n",
    "   The optimal coefficients can be obtained by going through the gradient ascent/descent process: initializing the beta terms, finding the derivative over each training example, and updating the weights accordingly. This continues until a local minima is reached or a set number of iterations have been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-3</h4> <br>\n",
    "In a fraud detection system, a QDA classifier‚Äôs confusion matrix is found to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        |Predicted Class - Not fraud| Predicted Class - fraud|\n",
    "|:--:|:--:|:--:|\n",
    "|Actual class ‚Äì Not fraud|1200|25|\n",
    "|Actual class ‚Äì fraud|30|7|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is dataset balanced? Why?\n",
    "- Evaluate the overall error rate <br>\n",
    "- Evaluate the precision and the recall <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The dataset is not balanced because there are many more cases of not fraud than fraud.\n",
    "2. Overall error rate is $\\frac{25 + 30}{1200 + 30 + 25 + 7} = 0.0436$.\n",
    "3. Precision is $\\frac{7}{7 + 25} = 0.21875$. \n",
    "\n",
    "   Recall is $\\frac{7}{7 + 30} = 0.1891$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem #4. SVM, Decision Trees, MLP Classification\n",
    "### [20 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will use different classification methods, SVM, KNN, Decision Tree and MLP; and find their accuracies using the test data. \n",
    "We will also use the Wisconson **\"breast_cancer\"** dataset.\n",
    "In all of the following subparts, use **random_state=0** when you split the dataset into train and test and **standardize** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCancer = load_breast_cancer()\n",
    "\n",
    "X = DataCancer.data\n",
    "Y = DataCancer.target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_t = scaler.transform(X_train)\n",
    "X_test_t = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-1.  Classification with SVM </h4>\n",
    "\n",
    "- How does the Radial Basis Function Kernel in SVM measure the similarity between a test point and a training example?\n",
    "- Fit an SVM classifier with a radial basis function kernel, with gamma =0.1, and regularization parameter C set to 10. Use the classifier to predict class labels for the test data. \n",
    "- Calculate the accuracy and confusion matrix on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel computes the squared Euclidean distance between the observation point and training point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9650.\n",
      "[[52  1]\n",
      " [ 4 86]]\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', gamma=0.1, C=10)\n",
    "svm.fit(X_train_t, Y_train)\n",
    "\n",
    "preds = svm.predict(X_test_t)\n",
    "acc = svm.score(X_test_t, Y_test)\n",
    "\n",
    "print('Accuracy is %.4f.' % acc)\n",
    "print(confusion_matrix(Y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-2.  Classification with Decisin Tree (DT) </h4>\n",
    "\n",
    "- In this part, use DT classification method on the training data. Set the maximum depth of the tree to five. Then use the classifier to predict class labels for the test data. Calculate the accuracy and confusion matrix on the test data.\n",
    "- Use Adaboost to combine four decision trees each of max_depth of five. Use random_state=0 in adaboost. Find the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8951.\n",
      "[[51  2]\n",
      " [13 77]]\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5, criterion='gini', random_state=0)\n",
    "tree.fit(X_train_t, Y_train)\n",
    "\n",
    "preds = tree.predict(X_test_t)\n",
    "acc = tree.score(X_test_t, Y_test)\n",
    "\n",
    "print('Accuracy is %.4f.' % acc)\n",
    "print(confusion_matrix(Y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9231.\n",
      "[[51  2]\n",
      " [ 9 81]]\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoostClassifier(n_estimators=5, random_state=0,\n",
    "                              base_estimator=DecisionTreeClassifier(max_depth=5, random_state=0))\n",
    "adaboost.fit(X_train_t, Y_train)\n",
    "preds = adaboost.predict(X_test_t)\n",
    "acc = adaboost.score(X_test_t, Y_test)\n",
    "\n",
    "print('Accuracy is %.4f.' % acc)\n",
    "print(confusion_matrix(Y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-3 </h4>\n",
    "\n",
    "Follow steps to answer questions to build a neural network using MLPClassifier from sklearn.neural_network. \n",
    "- Build a model that has two hidden layers, the first layer has 10 neurons and second layer has 5 neurons. \n",
    "- Use 'relu' activation function, and set the regularization parameter alpha=0.5. \n",
    "- Set max_iter=1000; Set the random_state=0.\n",
    "- Use stochastic gradient descent (sgd) to solve the optimization problem.\n",
    "- Report accuracy, confusion matrix, precision, and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52  1]\n",
      " [ 2 88]]\n",
      "Accuracy is 0.9790.\n",
      "Precision is 0.9888.\n",
      "Recall is 0.9778.\n"
     ]
    }
   ],
   "source": [
    "MLP = MLPClassifier(solver='sgd', activation='relu', random_state=0, max_iter=1000, hidden_layer_sizes=[10,5], alpha=0.5)\n",
    "MLP.fit(X_train_t, Y_train)\n",
    "\n",
    "preds = MLP.predict(X_test_t)\n",
    "acc = MLP.score(X_test_t, Y_test)\n",
    "\n",
    "print(confusion_matrix(Y_test, preds))\n",
    "print('Accuracy is %.4f.' % acc)\n",
    "print('Precision is %.4f.' % precision_score(Y_test, preds))\n",
    "print('Recall is %.4f.' % recall_score(Y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4> Problem #4-4 </h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same setting as **problem 4-3**, but instead of using two hidden layers, use three hidden layers with 10, 8, 5 hidden neurons respectively.\n",
    "- Find accuracy, precision, recall and confusion matrix. \n",
    "- Comment on the result comparing 4-3 and 4-4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is about the same with the three-hidden-layer model (slightly worse, even). This shows that the data has reached a point of diminishing returns, where it is no longer useful to add more layers to an MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  4]\n",
      " [ 3 87]]\n",
      "Accuracy is 0.9510.\n",
      "Precision is 0.9560.\n",
      "Recall is 0.9667.\n"
     ]
    }
   ],
   "source": [
    "MLP = MLPClassifier(solver='sgd', activation='relu', random_state=0, max_iter=1000, hidden_layer_sizes=[10,8,5], alpha=0.5)\n",
    "MLP.fit(X_train_t, Y_train)\n",
    "\n",
    "preds = MLP.predict(X_test_t)\n",
    "acc = MLP.score(X_test_t, Y_test)\n",
    "\n",
    "print(confusion_matrix(Y_test, preds))\n",
    "print('Accuracy is %.4f.' % acc)\n",
    "print('Precision is %.4f.' % precision_score(Y_test, preds))\n",
    "print('Recall is %.4f.' % recall_score(Y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem #5. Back propagation in Neural Networks\n",
    "### [25 points]\n",
    "\n",
    "In this problem, you should **not** use existing neural network libraries. You should build your own neural network to learn the XOR function. The network should have one hidden layer of three units.  You need to implement the network and have a function to fit the network (including implementing the backpropagation) and a function to predict the input after training. \n",
    "\n",
    "We need to train the network with multiple epochs on the XOR data (2 inputs X1 and X2, and one output Y). Please use sigmoid activation function at each layer (hidden and output layer). Initialize the weights randomly using a uniform distribution in the range [-1,1]. Set the learning rate to 0.1 (or less), and try number of epochs 200 (or more). \n",
    "\n",
    "Make reasonable assumptions as needed, but state them explicitly. Also, please comment your code to show your understanding of the algorithm.\n",
    "\n",
    "\n",
    "1. Plot the training mean squared error vs. epoch number\n",
    "\n",
    "2. After training, show the prediction of the 4 possible inputs to the XOR. For the prediction, the ouput of the last layer can be 0 if output is less than 0.5 and 1 otherwise. \n",
    "\n",
    "XOR data:\n",
    "\n",
    "(X1, X2, Y):\n",
    "\n",
    " (0 , 0 , 0)\n",
    " \n",
    " (0 , 1 , 1)\n",
    " \n",
    " (1 , 0,  1)\n",
    " \n",
    " (1 , 1,  0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x1, x2):\n",
    "    def sigmoid(x):\n",
    "      return 1 / (1 + np.e ** (-x))\n",
    "\n",
    "    sigmoid_v = np.vectorize(sigmoid)\n",
    "    \n",
    "    inp = np.array([x1, x2])\n",
    "    \n",
    "    HL_res = sigmoid_v(np.matmul(w1, inp) - np.transpose(b1)) \n",
    "\n",
    "    pred = sigmoid_v(np.matmul(w2, np.transpose(HL_res)) - b2)\n",
    "    \n",
    "    return HL_res, pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(x1, x2, HL_res, pred, y):\n",
    "    # HL_res is the intermediate result of the hidden layer neurons\n",
    "    y3 = HL_res[0][0]\n",
    "    y4 = HL_res[0][1]\n",
    "    y5 = HL_res[0][2]\n",
    "    \n",
    "    w36 = w2[0][0]\n",
    "    w46 = w2[0][1]\n",
    "    w56 = w2[0][2]\n",
    "    \n",
    "    # Get gradient for output neuron\n",
    "    del_6 = pred * (1 - pred) * (y - pred)\n",
    "    \n",
    "    # Get gradient for hidden layer neurons  \n",
    "    del_3 = y3 * (1 - y3) * del_6 * w36\n",
    "    del_4 = y4 * (1 - y4) * del_6 * w46\n",
    "    del_5 = y5 * (1 - y5) * del_6 * w56\n",
    "    \n",
    "    # Update weights for hidden layer neurons\n",
    "    w2[0][0] += lr * y3 * del_6\n",
    "    w2[0][1] += lr * y4 * del_6\n",
    "    w2[0][2] += lr * y5 * del_6\n",
    "    \n",
    "    # Bias term for output layer neuron\n",
    "    b2[0][0] += -1 * lr * del_6\n",
    "    \n",
    "    # Update weights for input layer neurons\n",
    "    w1[0][0] += lr * del_3 * x1\n",
    "    w1[0][1] += lr * del_3 * x2\n",
    "    \n",
    "    w1[1][0] += lr * del_4 * x1\n",
    "    w1[1][1] += lr * del_4 * x2\n",
    "    \n",
    "    w1[2][0] += lr * del_5 * x1\n",
    "    w1[2][1] += lr * del_5 * x2\n",
    "    \n",
    "    # Bias terms for hidden layer neurons\n",
    "    b1[0][0] += -1 * lr * del_3\n",
    "    b1[1][0] += -1 * lr * del_4\n",
    "    b1[2][0] += -1 * lr * del_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training data and hyperparameters\n",
    "X_train = [[0, 0, 1, 1], [0, 1, 0, 1]]\n",
    "Y_train = [0, 1, 1, 0]\n",
    "\n",
    "IL = 2\n",
    "HL = 3\n",
    "OL = 1\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Initialize weights and biases randomly, drawing from uniform distribution over [-1, 1]\n",
    "w1 = np.random.uniform(-1,1, (IL * HL)).reshape((HL, IL))\n",
    "w2 = np.random.uniform(-1,1, (HL * OL)).reshape((OL, HL))\n",
    "\n",
    "b1 = np.random.uniform(-1,1, (HL)).reshape((HL, 1))\n",
    "b2 = np.random.uniform(-1,1, (OL * 1)).reshape((OL, 1))\n",
    " \n",
    "epoch_num = []\n",
    "mse = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    epoch_num.append(epoch)\n",
    "    curr_mse = 0\n",
    "    \n",
    "    for x1, x2, y in zip(X_train[0], X_train[1], Y_train):\n",
    "        HL_res, pred = forward(x1, x2)\n",
    "        err = (y - pred) ** 2\n",
    "        curr_mse += err\n",
    "        \n",
    "        backpropagate(x1, x2, HL_res, pred, y)\n",
    "        \n",
    "    curr_mse /= len(X_train[0])\n",
    "    mse.append(curr_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2-D, but have shapes (10000,) and (10000, 1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1580d94eba58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot mean squared error vs. epoch number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch number'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean squared error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training MSE over time'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_new\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2759\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2760\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2761\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m   2763\u001b[0m         is not None else {}), **kwargs)\n",
      "\u001b[1;32m~\\anaconda3_new\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         \"\"\"\n\u001b[0;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1647\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1648\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_new\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3_new\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m             raise ValueError(f\"x and y can be no greater than 2-D, but have \"\n\u001b[0m\u001b[0;32m    346\u001b[0m                              f\"shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y can be no greater than 2-D, but have shapes (10000,) and (10000, 1, 1)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANQklEQVR4nO3cX2id933H8fdndg3rnzWhUUtnp9QbTlNfNCNR0zDWLV3ZamcXptCLpKVhoWDCmtLLhMHai9ysF4NSktSYYEJv6os1tO5IGwajzSBLFxlSJ05I0VwWay7EaUsHKSw4+e7inE1Cka3H5xxJjr7vFwj0nOcn6asf8tuPj3WeVBWSpO3vd7Z6AEnS5jD4ktSEwZekJgy+JDVh8CWpCYMvSU2sG/wkx5K8nOS5i5xPkm8kWUxyKsmNsx9TkjStIVf4jwAHLnH+ILBv/HYY+Ob0Y0mSZm3d4FfVE8CvLrHkEPCtGnkKuCrJ+2c1oCRpNnbO4HPsBs6uOF4aP/aL1QuTHGb0rwDe8Y533HT99dfP4MtLUh8nT558parmJvnYWQQ/azy25v0aquoocBRgfn6+FhYWZvDlJamPJP856cfO4rd0loBrVxzvAc7N4PNKkmZoFsE/Adw5/m2dW4DfVNWbns6RJG2tdZ/SSfJt4FbgmiRLwFeBtwFU1RHgMeA2YBH4LXDXRg0rSZrcusGvqjvWOV/AF2c2kSRpQ/hKW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5K8mGQxyX1rnH93ku8n+WmS00numv2okqRprBv8JDuAB4GDwH7gjiT7Vy37IvB8Vd0A3Ar8Q5JdM55VkjSFIVf4NwOLVXWmql4DjgOHVq0p4F1JArwT+BVwYaaTSpKmMiT4u4GzK46Xxo+t9ADwYeAc8Czw5ap6Y/UnSnI4yUKShfPnz084siRpEkOCnzUeq1XHnwKeAX4f+CPggSS/96YPqjpaVfNVNT83N3fZw0qSJjck+EvAtSuO9zC6kl/pLuDRGlkEfg5cP5sRJUmzMCT4TwP7kuwd/0fs7cCJVWteAj4JkOR9wIeAM7McVJI0nZ3rLaiqC0nuAR4HdgDHqup0krvH548A9wOPJHmW0VNA91bVKxs4tyTpMq0bfICqegx4bNVjR1a8fw74y9mOJkmaJV9pK0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqYlDwkxxI8mKSxST3XWTNrUmeSXI6yY9nO6YkaVo711uQZAfwIPAXwBLwdJITVfX8ijVXAQ8BB6rqpSTv3aiBJUmTGXKFfzOwWFVnquo14DhwaNWazwKPVtVLAFX18mzHlCRNa0jwdwNnVxwvjR9b6Trg6iQ/SnIyyZ1rfaIkh5MsJFk4f/78ZBNLkiYyJPhZ47FadbwTuAn4K+BTwN8lue5NH1R1tKrmq2p+bm7usoeVJE1u3efwGV3RX7vieA9wbo01r1TVq8CrSZ4AbgB+NpMpJUlTG3KF/zSwL8neJLuA24ETq9Z8D/h4kp1J3g58DHhhtqNKkqax7hV+VV1Icg/wOLADOFZVp5PcPT5/pKpeSPJD4BTwBvBwVT23kYNLki5PqlY/Hb855ufna2FhYUu+tiS9VSU5WVXzk3ysr7SVpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpiUHBT3IgyYtJFpPcd4l1H03yepLPzG5ESdIsrBv8JDuAB4GDwH7gjiT7L7Lua8Djsx5SkjS9IVf4NwOLVXWmql4DjgOH1lj3JeA7wMsznE+SNCNDgr8bOLvieGn82P9Lshv4NHDkUp8oyeEkC0kWzp8/f7mzSpKmMCT4WeOxWnX8deDeqnr9Up+oqo5W1XxVzc/NzQ2dUZI0AzsHrFkCrl1xvAc4t2rNPHA8CcA1wG1JLlTVd2cypSRpakOC/zSwL8le4L+A24HPrlxQVXv/7/0kjwD/ZOwl6cqybvCr6kKSexj99s0O4FhVnU5y9/j8JZ+3lyRdGYZc4VNVjwGPrXpszdBX1V9PP5YkadZ8pa0kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMmLSRaT3LfG+c8lOTV+ezLJDbMfVZI0jXWDn2QH8CBwENgP3JFk/6plPwf+rKo+AtwPHJ31oJKk6Qy5wr8ZWKyqM1X1GnAcOLRyQVU9WVW/Hh8+BeyZ7ZiSpGkNCf5u4OyK46XxYxfzBeAHa51IcjjJQpKF8+fPD59SkjS1IcHPGo/VmguTTzAK/r1rna+qo1U1X1Xzc3Nzw6eUJE1t54A1S8C1K473AOdWL0ryEeBh4GBV/XI240mSZmXIFf7TwL4ke5PsAm4HTqxckOQDwKPA56vqZ7MfU5I0rXWv8KvqQpJ7gMeBHcCxqjqd5O7x+SPAV4D3AA8lAbhQVfMbN7Yk6XKlas2n4zfc/Px8LSwsbMnXlqS3qiQnJ72g9pW2ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHkxyWKS+9Y4nyTfGJ8/leTG2Y8qSZrGusFPsgN4EDgI7AfuSLJ/1bKDwL7x22HgmzOeU5I0pSFX+DcDi1V1pqpeA44Dh1atOQR8q0aeAq5K8v4ZzypJmsLOAWt2A2dXHC8BHxuwZjfwi5WLkhxm9C8AgP9J8txlTbt9XQO8stVDXCHci2XuxTL3YtmHJv3AIcHPGo/VBGuoqqPAUYAkC1U1P+Drb3vuxTL3Ypl7scy9WJZkYdKPHfKUzhJw7YrjPcC5CdZIkrbQkOA/DexLsjfJLuB24MSqNSeAO8e/rXML8Juq+sXqTyRJ2jrrPqVTVReS3AM8DuwAjlXV6SR3j88fAR4DbgMWgd8Cdw342kcnnnr7cS+WuRfL3Itl7sWyifciVW96ql2StA35SltJasLgS1ITGx58b8uwbMBefG68B6eSPJnkhq2YczOstxcr1n00yetJPrOZ822mIXuR5NYkzyQ5neTHmz3jZhnwZ+TdSb6f5KfjvRjy/4VvOUmOJXn5Yq9VmribVbVhb4z+k/c/gD8AdgE/BfavWnMb8ANGv8t/C/CTjZxpq94G7sUfA1eP3z/YeS9WrPsXRr8U8JmtnnsLfy6uAp4HPjA+fu9Wz72Fe/G3wNfG788BvwJ2bfXsG7AXfwrcCDx3kfMTdXOjr/C9LcOydfeiqp6sql+PD59i9HqG7WjIzwXAl4DvAC9v5nCbbMhefBZ4tKpeAqiq7bofQ/aigHclCfBORsG/sLljbryqeoLR93YxE3Vzo4N/sVsuXO6a7eByv88vMPobfDtady+S7AY+DRzZxLm2wpCfi+uAq5P8KMnJJHdu2nSba8hePAB8mNELO58FvlxVb2zOeFeUibo55NYK05jZbRm2gcHfZ5JPMAr+n2zoRFtnyF58Hbi3ql4fXcxtW0P2YidwE/BJ4HeBf0vyVFX9bKOH22RD9uJTwDPAnwN/CPxzkn+tqv/e6OGuMBN1c6OD720Zlg36PpN8BHgYOFhVv9yk2TbbkL2YB46PY38NcFuSC1X13c0ZcdMM/TPySlW9Crya5AngBmC7BX/IXtwF/H2NnsheTPJz4Hrg3zdnxCvGRN3c6Kd0vC3DsnX3IskHgEeBz2/Dq7eV1t2LqtpbVR+sqg8C/wj8zTaMPQz7M/I94ONJdiZ5O6O71b6wyXNuhiF78RKjf+mQ5H2M7hx5ZlOnvDJM1M0NvcKvjbstw1vOwL34CvAe4KHxle2F2oZ3CBy4Fy0M2YuqeiHJD4FTwBvAw1W17W4tPvDn4n7gkSTPMnpa496q2na3TU7ybeBW4JokS8BXgbfBdN301gqS1ISvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5Ka+F/Xe3Wlc9XddQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot mean squared error vs. epoch number\n",
    "plt.scatter(epoch_num, mse)\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title('Training MSE over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = 0. X2 = 0. Prediction = 0.\n",
      "X1 = 0. X2 = 1. Prediction = 1.\n",
      "X1 = 1. X2 = 0. Prediction = 1.\n",
      "X1 = 1. X2 = 1. Prediction = 0.\n"
     ]
    }
   ],
   "source": [
    "def predict():\n",
    "    X_test = [[0, 0, 1, 1], [0, 1, 0, 1]]\n",
    "    for x1, x2 in zip(X_test[0], X_test[1]):\n",
    "        res1, pred = forward(x1, x2)\n",
    "        if pred > 0.5:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = 0\n",
    "        print(f'X1 = {x1}. X2 = {x2}. Prediction = {pred}.')\n",
    "        \n",
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem #6. Neural Network\n",
    "### [20 points]\n",
    "Apply neural networks (multilayer perceptron) to classify the Iris species, and build a model that has two hidden layers, the first layer has 10 neurons and second layer has 5 neurons. Use 'tanh' activation function, and set the regularization parameter alpha=0.5. Scale the feautures with MinMaxScaler. Try the following settings (a)-(c) and report the accuracy, then comment on the results. \n",
    "\n",
    "**Note:** please make any reasonable assumptions as needed, but state them explicitly. Also, please comment your code to show your understanding of the algorithm.\n",
    "\n",
    "a) Use gradient descent to solve the optimization  problem, and choose random_state=0 (which corresponds to a particular initialization of weight values), and set max_iter=5000. Print the test accuracy.\n",
    "   \n",
    "b) Repeat (a) above but with a model that uses random_state=10 to initialize the weights. Print the test accuracy.\n",
    "    \n",
    "    \n",
    "c) Repeat (b) but with model that use L-BFGS (a numerical quasi-Newton method of optimization) instead of stochastic gradient descent to find the weights. Print the test accuracy\n",
    "    \n",
    "d) Comment on results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IrisData = load_iris()\n",
    "\n",
    "X = IrisData.data\n",
    "Y = IrisData.target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train_t = scaler.transform(X_train)\n",
    "X_test_t = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9474.\n"
     ]
    }
   ],
   "source": [
    "MLP = MLPClassifier(solver='sgd', activation='tanh', random_state=0, max_iter=5000, hidden_layer_sizes=[10,5], alpha=0.5)\n",
    "MLP.fit(X_train_t, Y_train)\n",
    "\n",
    "preds = MLP.predict(X_test_t)\n",
    "acc = MLP.score(X_test_t, Y_test)\n",
    "\n",
    "print('Accuracy is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9211.\n"
     ]
    }
   ],
   "source": [
    "MLP = MLPClassifier(solver='sgd', activation='tanh', random_state=10, max_iter=5000, hidden_layer_sizes=[10,5], alpha=0.5)\n",
    "MLP.fit(X_train_t, Y_train)\n",
    "\n",
    "preds = MLP.predict(X_test_t)\n",
    "acc = MLP.score(X_test_t, Y_test)\n",
    "\n",
    "print('Accuracy is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9737.\n"
     ]
    }
   ],
   "source": [
    "MLP = MLPClassifier(solver='lbfgs', activation='tanh', random_state=10, max_iter=5000, hidden_layer_sizes=[10,5], alpha=0.5)\n",
    "MLP.fit(X_train_t, Y_train)\n",
    "\n",
    "preds = MLP.predict(X_test_t)\n",
    "acc = MLP.score(X_test_t, Y_test)\n",
    "\n",
    "print('Accuracy is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between A and B seems to be relatively small since it is just a difference of how the random weights are initialized. This likely means that, in part B, the gradient descent is finding a local minimum instead of the global minimum. A is not necessarily the global minimum, but it is more of a minimum than the one found in Part B. Using the L-BFGS solver in Part C has improved the overall accuracy, meaning that it is closer to the global minimum of the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
