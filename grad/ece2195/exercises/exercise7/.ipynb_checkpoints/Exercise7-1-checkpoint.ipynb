{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Decision Trees and Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Please use the Iris dataset for this part.\n",
    "Data can be imported as follows\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "Let's use random_state=0 for splitting and building all models.\n",
    "\n",
    "#### 1) Fit decision tree with maximum depth (max_depth) of 2 and the default gini index for building the  tree. Find the model accuracy (with test data). \n",
    "\n",
    "- To visualize the tree (optional), first, import the graphviz package from terminal using the following:\n",
    "\n",
    "        brew install graphviz\n",
    "\n",
    "        OR\n",
    "\n",
    "        #conda install -c anaconda graphviz  \n",
    "        #conda install -c anaconda python-graphviz \n",
    "\n",
    "    Then, we can use the package to visulaize the decision tree as follows: \n",
    "        \n",
    "        from sklearn.tree import export_graphviz\n",
    "        import graphviz \n",
    "        \n",
    "         dot_data=export_graphviz(FittedTreeModel,class_names=dataset.target_names,   feature_names=dataset.feature_names,out_file=None)\n",
    "        \n",
    "       graph = graphviz.Source(dot_data)  \n",
    "       graph \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8947.\n"
     ]
    }
   ],
   "source": [
    "iris_dataset = load_iris()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
    "\n",
    "m = 2\n",
    "treeModel = DecisionTreeClassifier(max_depth=m)\n",
    "treeModel.fit(X_train, Y_train)\n",
    "\n",
    "preds = treeModel.predict(X_test)\n",
    "acc = np.count_nonzero(preds==Y_test) / len(preds)\n",
    "print('Accuracy is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Use random forests for classification of Iris examples. The random forests combines 4 decision trees, each of maximum depth 2 and maximum number of features considered at each split is 2. What is the model accuracy? Compare preformance to previous part.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9474.\n"
     ]
    }
   ],
   "source": [
    "forestModel = RandomForestClassifier(n_estimators=4, max_features=2, max_depth=2, random_state=0)\n",
    "forestModel.fit(X_train, Y_train)\n",
    "\n",
    "preds = forestModel.predict(X_test)\n",
    "acc = np.count_nonzero(preds==Y_test) / len(preds)\n",
    "print('Accuracy is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(AP)**: The accuracy for the random forests is improved by ~6 percentage points over the regular decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Use AdaBoost with 4 decision tree models to perform the classification of the Iris species. What is the accuracy?  Increase the number of models from 4 to 14. Find the accuracy. Comment on the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 4 models is 0.9737.\n",
      "Accuracy for 5 models is 0.9737.\n",
      "Accuracy for 6 models is 0.8947.\n",
      "Accuracy for 7 models is 0.8947.\n",
      "Accuracy for 8 models is 0.9737.\n",
      "Accuracy for 9 models is 0.9737.\n",
      "Accuracy for 10 models is 0.8947.\n",
      "Accuracy for 11 models is 0.8947.\n",
      "Accuracy for 12 models is 0.9737.\n",
      "Accuracy for 13 models is 0.9737.\n",
      "Accuracy for 14 models is 0.8947.\n"
     ]
    }
   ],
   "source": [
    "for m in range(4, 15):\n",
    "    boostModel = AdaBoostClassifier(n_estimators=m)\n",
    "    boostModel.fit(X_train, Y_train)\n",
    "    \n",
    "    preds = boostModel.predict(X_test)\n",
    "    acc = np.count_nonzero(preds==Y_test) / len(preds)\n",
    "    print('Accuracy for ' + str(m) + ' models is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(AP)**: The accuracy seems to oscillate between 89% and 97% as the number of models are increased. This is likely because the dataset is not big enough to support adding several extra models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4) Use Bagging with 4 decision tree models, and find the accuracy. Repeat with 14 models. Do you think it will overfit as the number of base models increases? Comment on results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 4 models is 0.9211.\n",
      "Accuracy for 14 models is 0.9737.\n"
     ]
    }
   ],
   "source": [
    "for m in [4, 14]:\n",
    "    baggingModel = BaggingClassifier(n_estimators=m)\n",
    "    baggingModel.fit(X_train, Y_train)\n",
    "\n",
    "    preds = baggingModel.predict(X_test)\n",
    "    acc = np.count_nonzero(preds==Y_test) / len(preds)\n",
    "    print('Accuracy for ' + str(m) + ' models is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(AP)**: Bagging is not generally susceptible to overfitting as the number of estimators increase, because all of the models are independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  5) Use Bagging with 4 LDA base classifier instead of the default decision tree classifier, and find the accuracy.\n",
    "Hints:  \n",
    "-Change input argument of BaggingClassifier as follows base_estimator=LDA( ) \n",
    "\n",
    "-Need to add: import \"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9737.\n"
     ]
    }
   ],
   "source": [
    "baggingModel = BaggingClassifier(n_estimators=4, base_estimator=LDA())\n",
    "baggingModel.fit(X_train, Y_train)\n",
    "\n",
    "preds = baggingModel.predict(X_test)\n",
    "acc = np.count_nonzero(preds==Y_test) / len(preds)\n",
    "print('Accuracy is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Adaboost algorithm\n",
    "In this part we will implement the Adaboost algorithm without using the scikit-learn AdaBoostClassifier method. We can however use the DecisionTreeClassifier of Scikit-learn to build the base models. \n",
    "We will use synthetic data below of two classes and 4 features. Implement the Adaboost Algorithm using two base models that are Decision Tree classifiers, each has a single split only. \n",
    "Find the accuracy using the test data and verify the accuracy with that obtained using AdaBoostClassifier function of scikit-learn.\n",
    "- Please split the X and Y given below into train and test sets. We will use the train set to build and fit the models, then evaluate with samples from the test set. Let's use random_state =0\n",
    "- Initialize the weights of the training data based on the size of the training set. If the size of the training set is n, then the initial weight of each sample is 1/n.\n",
    "- We will have two base models, each is a Decision Tree classifier with a single split. \n",
    "- Following the Adaboost algorithm discussed in the lecture, you would need to evaluate weights of each sample as well as weight of each base model. \n",
    "- Note that the .fit method takes training data and can take weight vector for each sample (model.fit(X_train,Y_train,sample_weight=weights)). \n",
    "- After fitting each of the base models, and evaluating each model weight, predict the first sample in the test data (first sample is at index 0). You will need to write the code for combining the predicitions of the base models using their weights. Compare the final prediction of the first test example with the actual label to see if it is correct or not. \n",
    "- Find the overall accuracy using all the test data and verify using the built-in AdaBoostClassifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#creating synthetic data from 2 classes and 4 features\n",
    "X, Y = make_classification(n_samples=10000, n_features=4, n_classes=2, n_clusters_per_class=1, weights=[0.5], random_state=0)\n",
    "Y=2*Y-1 # this will just make class clabels encoded as 1 and -1 instead of 0 and 1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "weights = [1/len(X_train)] * len(X_train)\n",
    "alphas = []\n",
    "clfs = [DecisionTreeClassifier(max_depth=1, random_state=0)] * 2\n",
    "\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train, Y_train, sample_weight=weights)\n",
    "    preds = clf.predict(X_train)\n",
    "    \n",
    "    # compute weighted error\n",
    "    err = 0\n",
    "    for wt, y_est, y in zip(weights, preds, Y_train):\n",
    "        if y_est != y:\n",
    "            err += wt\n",
    "    \n",
    "    # calculate alpha\n",
    "    alpha = 0.5 * np.log((1-err) / err)\n",
    "    alphas.append(alpha)\n",
    "    \n",
    "    # update weights\n",
    "    for i in range(0, len(X_train)):\n",
    "        if preds[i] != Y_train[i]:\n",
    "            mult = -1\n",
    "        else:\n",
    "            mult = 1\n",
    "        weights[i] = weights[i] * np.exp(-1 * alpha * mult)\n",
    "   \n",
    "    # normalize weights\n",
    "    tot = sum(weights)\n",
    "    weights = [wt / tot for wt in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.964776791336334, 0.16963237563183378]\n",
      "For the first test sample, the prediction is class -1. Label is class 1.\n"
     ]
    }
   ],
   "source": [
    "# evaluate on first test sample\n",
    "print(alphas) # the alpha values agree with the classmate I asked\n",
    "running_total = 0 \n",
    "preds = [clf.predict(X_test) for clf in clfs]\n",
    "\n",
    "for alpha, pred in zip(alphas, preds):\n",
    "    running_total += alpha * pred[0]\n",
    "    \n",
    "if running_total > 0:\n",
    "    est = 1\n",
    "else:\n",
    "    est = -1\n",
    "    \n",
    "print(f'For the first test sample, the prediction is class {est}. Label is class {Y_test[0]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.7540.\n"
     ]
    }
   ],
   "source": [
    "# evaluate on all test samples\n",
    "disc_preds = []\n",
    "\n",
    "for i in range(0, len(X_test)):\n",
    "    running_total = 0\n",
    "    for alpha, pred in zip(alphas, preds):\n",
    "        running_total += alpha * pred[i]\n",
    "    \n",
    "    if running_total > 0:\n",
    "        disc_preds.append(1)\n",
    "    else:\n",
    "        disc_preds.append(-1)\n",
    "    \n",
    "acc = np.count_nonzero(disc_preds==Y_test) / len(disc_preds)\n",
    "print('Accuracy is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 ...  1 -1 -1]\n",
      "Accuracy is 0.8808.\n"
     ]
    }
   ],
   "source": [
    "# verify using built-in AdaBoostClassifier\n",
    "boostModel = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1, random_state=0), \n",
    "                                n_estimators=2, random_state=0)\n",
    "boostModel.fit(X_train, Y_train)\n",
    "\n",
    "preds = boostModel.predict(X_test)\n",
    "print(preds)\n",
    "acc = np.count_nonzero(preds==Y_test) / len(preds)\n",
    "print('Accuracy is %.4f.' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
